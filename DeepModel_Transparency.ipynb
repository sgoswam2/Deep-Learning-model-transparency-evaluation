{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepModel_Transparency.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPd3cBK/Rc9ECN7xScUpEko"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TAf6U_5BYOKr"},"source":["Model Transparency/ Explainability"]},{"cell_type":"markdown","metadata":{"id":"ADxyMTpDZLtF"},"source":["Compute shapley values with SHAP'S DeepExplainer class"]},{"cell_type":"code","metadata":{"id":"E-QybzTTYHRI"},"source":["import shap"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeyvM86ianNI"},"source":["Here we are using 100 training samples (distrib_samples) and validating on 30 samples (num_explanations) starting from 3000 index (start). Assume model has three inputs, two unstructured texts and one structured text"]},{"cell_type":"code","metadata":{"id":"ErGsO8uvaiCj"},"source":["shap.initjs()\n","import keras\n","# select a set of samples to take an expectation over\n","distrib_samples=[unstructured_text1_train[:100],unstructured_text2_train[:100],structured_train[:100]]\n","session=keras.backend.tensorflow_backend.get_session()\n","\n","explainer=shap.DeepExplainer(model, distrib_samples, session)\n","num_explanations=30\n","start=3000\n","\n","shap_values=explainer.shap_values([unstructured_text1_valid[start:start+num_explanations],unstructured_text2_valid[start:start+num_explanations],structured_valid[start:start+num_explanations]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xCIWl0JKhfd6"},"source":["Generalized model transparency using shap.summary_plot"]},{"cell_type":"code","metadata":{"id":"IFyddskqhn2e"},"source":["num2word_text1={}\n","num2word_text2={}\n","num2word_structured={}\n","\n","### get all words\n","structured_fields=['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','f12','f13','f14']\n","word_index_text1= tokenizer_text1.word_index\n","word_index_text2= tokenizer_text2.word_index\n","for w in word_index_text1.keys():\n","  num2word_text1=[word_index_text1[w]]=w\n","for w in word_index_text2.keys():\n","  num2word_text2=[word_index_text2[w]]=w\n","\n","### summary plot for each of the inputs\n","shap.summary_plot(np,.concatenate([shap_values[0][2]],axis=1), feature_names=structured_fields, max_display=15)\n","shap.summary_plot(np,.concatenate([shap_values[0][0]],axis=1), feature_names= list(num2word_text1.values()), max_display=15)\n","shap.summary_plot(np,.concatenate([shap_values[0][1]],axis=1), feature_names= list(num2word_text2.values()), max_display=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFfVSnLMuRVN"},"source":["### here we will find shap.force_plot for each test output\n","### find shap.force_plot for 2nd valid data\n","i=2\n","test_inp=[unstructured_text1_valid[start+i].reshape(1,-1),unstructured_text2_valid[start+i].reshape(1,-1),structured_valid[start+i].reshape(1,-1)] \n","pred=model.predict(test_inp)\n","print(\"model outputs: {}\".format(pred))\n","x_test_words1=np.asarray(list(map(lambda x: num2word_text1.get(x,\"None\"),test_inp[0][0].tolist())))\n","x_test_words2=np.asarray(list(map(lambda x: num2word_text2.get(x,\"None\"),test_inp[1][0].tolist())))\n","\n","x_test_words=[]\n","for j in range(len(x_test_words1)):\n","  x_test_words.append(x_test_words1[j])\n","for j in range(len(x_test_words2)):\n","  x_test_words.append(x_test_words2[j])\n","x_test_words=np.asarray(x_test_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5iWMUapLy2Ob"},"source":["Get positive features which improves model performance and also negative features that reduces model performance"]},{"cell_type":"code","metadata":{"id":"QqWGZwrC0MDq"},"source":["import nltk\n","shap_user=np.concatenate([shap_values[0][0],shap_values[0][1]], axis=1)[i]\n","shap_user_importance=np.argsort(shap_user)\n","\n","## Want to see top 10 users\n","top_user_n=10\n","\n","### Find most negative influence words\n","neg_cols=[x_test_words[shap_user_importance[c]] for c in range(top_user_n)]\n","### Get all neg_ind for words except pronouns, prepositions\n","indexes_neg=[neg_cols.index(x) for x in set(neg_cols) if (nltk.pos_tag(nltk.word_tokenize(x))[0][1] !='PRP' and nltk.pos_tag(nltk.word_tokenize(x))[0][1] !='IN')]\n","neg_cols=[neg_cols[i] for i in indexes_neg]\n","neg_vals=[shap_user[shap_user_importance[c]] for c in range(top_user_n)]\n","neg_vals=[neg_vals[i] for i in indexes_neg]\n","neg_indexes=[shap_user_importance[c] for c in range(top_user_n)]\n","neg_indexes=[neg_indexes[i] for i in index_neg]\n","\n","### Find most positive influence words\n","pos_cols=[x_test_words[shap_user_importance[-(c+1)]] for c in range(top_user_n)]\n","### Get all neg_ind for words except pronouns, prepositions\n","indexes_pos=[pos_cols.index(x) for x in set(pos_cols) if (nltk.pos_tag(nltk.word_tokenize(x))[0][1] !='PRP' and nltk.pos_tag(nltk.word_tokenize(x))[0][1] !='IN')]\n","pos_cols=[pos_cols[i] for i in indexes_pos]\n","pos_vals=[shap_user[shap_user_importance[c]] for c in range(top_user_n)]\n","pos_vals=[pos_vals[i] for i in indexes_pos]\n","pos_indexes=[shap_user_importance[-(c+1)] for c in range(top_user_n)]\n","pos_indexes=[pos_indexes[i] for i in index_pos]\n","\n","print('positive features', neg_cols,neg_vals,neg_indexes)\n","print('negative features', pos_cols,pos_vals,pos_indexes)\n","main_feats=neg_indexes[:]\n","main_feats.extend(pos_indexes[:])\n","main_col_names=[x_test_words[k] for k in main_feats]\n","shap.force_plot(explainer.expected_value[0], shap_user[main_feats], feature_names=main_col_names, text_rotation=90, figsize=(60,60))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_9cTxV_39rjf"},"source":["Get the snippets (past 5 words and next 5 words) for each positive & negative model features"]},{"cell_type":"code","metadata":{"id":"QpmzmeUV945q"},"source":["snippets_neg=[]\n","for n1 in range(len(neg_cols)):\n","  l1=np.where(x_test_words1 == neg_cols[n1])[0]\n","  if np.shape(l1)[0]>0:\n","    l2=l1[0]\n","    snippets_neg.append(' '.join(x_test_words1[l2-5:l2+5]))\n","\n","snippets_pos=[]\n","for n1 in range(len(pos_cols)):\n","  l1=np.where(x_test_words1 == pos_cols[n1])[0]\n","  if np.shape(l1)[0]>0:\n","    l2=l1[0]\n","    snippets_pos.append(' '.join(x_test_words1[l2-5:l2+5]))"],"execution_count":null,"outputs":[]}]}